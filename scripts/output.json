[
    {
        "block_type": "BigQueryReader",
        "code_text": "from serra.readers import Reader\n\n\nclass BigQueryReader(Reader):\n    \"\"\"\n    A reader to read data from BigQuery into a Spark DataFrame.\n\n    :param config: A dictionary containing the configuration for the reader.\n                   It should have the following keys:\n                   - 'project'\n                   - 'dataset'\n                   - 'table'\n    \"\"\"\n\n    def __init__(self, project, dataset, table, mode):\n        self.project = project\n        self.dataset = dataset\n        self.table = table\n        self.mode = mode\n\n    def read(self):\n        \"\"\"\n        Read data from Snowflake and return a Spark DataFrame.\n\n        :return: A Spark DataFrame containing the data read from the specified Snowflake table.\n        \"\"\"\n        df = self.spark.read.format(\"bigquery\")\\\n            .option('parentProject', self.project)\\\n            .load(f\"{self.dataset}.{self.table}\")\n        return df\n"
    },
    {
        "block_type": "LocalReader",
        "code_text": "from serra.readers import Reader\n\n\nclass LocalReader(Reader):\n    \"\"\"\n    A reader to read data from a local file into a Spark DataFrame.\n\n    :param config: A dictionary containing the configuration for the reader.\n                   It should have the following key:\n                   - 'file_path': The path to the local file to be read.\n    \"\"\"\n\n\n    def __init__(self, file_path):\n        self.file_path = file_path\n        \n    def read(self):\n        \"\"\"\n        Read data from a local file and return a Spark DataFrame.\n\n        :return: A Spark DataFrame containing the data read from the local file.\n        \"\"\"\n        #TODO: check all files supports\n        df = self.spark.read.format(\"csv\").option(\"header\",True).load(self.file_path)\n        return df\n\n"
    },
    {
        "block_type": "S3Reader",
        "code_text": "from serra.readers import Reader\n\nclass S3Reader(Reader):\n    \"\"\"\n    A class to read data from Amazon S3 and return a Spark DataFrame.\n\n    :param config: A dictionary containing the configuration for the reader.\n                   It should have the following keys:\n                   - 'bucket_name': The name of the S3 bucket containing the file.\n                   - 'file_path': The path to the file within the S3 bucket.\n                   - 'file_type': The type of file to be read (e.g., 'csv', 'parquet').\n    \"\"\"\n\n    def __init__(self, bucket_name, file_path, file_type, options):\n        self.bucket_name = bucket_name\n        self.file_path = file_path\n        self.file_type = file_type\n        self.options = options\n    \n    def read(self):\n        \"\"\"\n        Read data from Amazon S3 and return a Spark DataFrame.\n\n        :return: A Spark DataFrame containing the data read from the S3 bucket.\n        :raises: Any exceptions that may occur during file reading or DataFrame creation.\n        \"\"\"\n        spark = self.spark\n        s3_url = f\"s3a://{self.bucket_name}/{self.file_path}\"\n        df_read = spark.read\n\n        # To specify options like header: true\n        if self.options is not None:\n            for option_key, option_value in self.options.items():\n                df_read = df_read.option(option_key, option_value)\n            \n        df = df_read.format(self.file_type).load(s3_url)\n\n        return df\n"
    },
    {
        "block_type": "DatabricksReader",
        "code_text": "from serra.readers import Reader\nfrom serra.exceptions import SerraRunException\n\nclass DatabricksReader(Reader):\n    \"\"\"\n    A reader to read data from a Databricks Delta Lake table into a Spark DataFrame.\n\n    :param config: A dictionary containing the configuration for the reader.\n                   It should have the following keys:\n                   - 'database': The name of the database containing the table.\n                   - 'table': The name of the table to be read.\n    \"\"\"\n\n    def __init__(self, database, table):\n        self.database = database\n        self.table = table\n\n    def read(self):\n        \"\"\"\n        Read data from a Databricks Delta Lake table and return a Spark DataFrame.\n\n        :return: A Spark DataFrame containing the data read from the specified table.\n        :raises: SerraRunException if an error occurs during the data reading process.\n        \"\"\"\n        try:\n            df = self.spark.read.table(f'{self.database}.{self.table}')\n        except Exception as e:\n            raise SerraRunException(e)\n        return df\n"
    },
    {
        "block_type": "MongoDbReader",
        "code_text": "from serra.readers import Reader\n\nclass MongoDBReader(Reader):\n    def __init__(self, username, password, database, collection, cluster_ip_and_options):\n        self.username = username\n        self.password = password\n        self.database = database\n        self.collection = collection\n        self.cluster_ip_and_options = cluster_ip_and_options\n\n    def read(self):\n        spark = self.spark\n\n        #TODO: connection.uri can either start iwth mongodb+srv or mongodb\n\n        \"\"\"\n        Cluster must have this package installed:\n        org.mongodb.spark:mongo-spark-connector_2.12:10.2.1\n\n        Must also run Spark 3.1 - 3.2.4 according to https://www.mongodb.com/docs/spark-connector/v10.2/\n        \"\"\"\n\n        df = spark.read\\\n        .format(\"mongodb\")\\\n        .option(\"connection.uri\", f'mongodb+srv://{self.username}:{self.password}@{self.cluster_ip_and_options}')\\\n        .option(\"database\", self.database)\\\n        .option(\"collection\", self.collection)\\\n        .load()\n        return df"
    },
    {
        "block_type": "SnowflakerReader",
        "code_text": "from serra.readers import Reader\n\nclass SnowflakeReader(Reader):\n    \"\"\"\n    A reader to read data from Snowflake into a Spark DataFrame.\n\n    :param config: A dictionary containing the configuration for the reader.\n                   It should have the following keys:\n                   - 'warehouse': The Snowflake warehouse to use for the connection.\n                   - 'database': The Snowflake database to use for the connection.\n                   - 'schema': The Snowflake schema to use for the connection.\n                   - 'table': The name of the table to be read from Snowflake.\n    \"\"\"\n\n    def __init__(self, warehouse, database, schema, table, user, password, host):\n        self.warehouse = warehouse\n        self.database = database\n        self.schema = schema\n        self.table = table\n        self.user = user\n        self.password = password\n        self.host = host\n    \n    def read(self):\n        \"\"\"\n        Read data from Snowflake and return a Spark DataFrame.\n\n        :return: A Spark DataFrame containing the data read from the specified Snowflake table.\n        \"\"\"\n        snowflake_table = (self.spark.read\n        .format(\"snowflake\")\n        .option(\"host\", self.host)\n        .option(\"user\", self.user)\n        .option(\"password\", self.password)\n        .option(\"sfWarehouse\", self.warehouse)\n        .option(\"database\", self.database)\n        .option(\"schema\", self.schema) # Optional - will use default schema \"public\" if not specified.\n        .option(\"dbtable\", self.table)\n        .load()\n        )\n        return snowflake_table"
    },
    {
        "block_type": "LocalWriter",
        "code_text": "from serra.writers.writer import Writer\n\nclass LocalWriter(Writer):\n    \"\"\"\n    A writer to write data from a Spark DataFrame to a local file.\n\n    :param file_path: The path of the local file to write to.\n    \"\"\"\n\n    def __init__(self, file_path):\n        self.file_path = file_path\n\n    def write(self, df):\n        \"\"\"\n        Write data from a Spark DataFrame to a local file.\n\n        :param df: The Spark DataFrame to be written to the local file.\n        \"\"\"\n        # Convert PySpark DataFrame to Pandas DataFrame\n        pandas_df = df.toPandas()\n\n        # Write the Pandas DataFrame to a local file\n        pandas_df.to_csv(self.file_path, index=False)"
    },
    {
        "block_type": "BigQueryWriter",
        "code_text": "from google.cloud import bigquery\n\nfrom serra.exceptions import SerraRunException\nfrom serra.writers import Writer\n\nclass BigQueryWriter(Writer):\n    \"\"\"\n    A writer to write data to BigQuery from a Spark DataFrame.\n\n    :param config: A dictionary containing the configuration for the reader.\n                   It should have the following keys:\n                   - 'project_id'\n                   - 'dataset_id'\n                   - 'table_id'\n    \"\"\"\n\n    def __init__(self, project_id, dataset_id, table_id, mode):\n        self.project_id = project_id\n        self.dataset_id = dataset_id\n        self.table_id = table_id\n        self.mode = mode\n    \n    def write(self, df):\n        \"\"\"\n        Read data from Snowflake and return a Spark DataFrame.\n\n        :return: A Spark DataFrame containing the data read from the specified Snowflake table.\n        \"\"\"\n        df.write \\\n            .format(\"bigquery\") \\\n            .option('project', self.project_id)\\\n            .option(\"writeMethod\", \"direct\") \\\n            .mode(self.mode)\\\n            .save(f\"{self.dataset_id}.{self.table_id}\")\n"
    },
    {
        "block_type": "SnowflakeWriter",
        "code_text": "from loguru import logger\n\nfrom serra.writers import Writer\n\nclass SnowflakeWriter(Writer):\n    \"\"\"\n    A writer to write data from a Spark DataFrame to a Snowflake table.\n\n    :param config: A dictionary containing the configuration for the writer.\n                   It should have the following keys:\n                   - 'type': The type of operation to perform. Possible values are 'create' or 'insert'.\n                   - 'warehouse': The Snowflake warehouse to use for the connection.\n                   - 'database': The name of the Snowflake database to write to.\n                   - 'schema': The name of the Snowflake schema to write to.\n                   - 'table': The name of the Snowflake table to write to.\n    \"\"\"\n\n    def __init__(self, warehouse, database, schema, table, host, user, password, mode):\n        self.warehouse = warehouse\n        self.database = database\n        self.schema = schema\n        self.table = table\n        self.host = host\n        self.user = user\n        self.password = password\n        self.mode = mode\n    \n    def write(self, df):\n        \"\"\"\n        Write data from a Spark DataFrame to a Snowflake table.\n        :param df: The Spark DataFrame to be written to the Snowflake table.\n        \"\"\"\n        assert self.mode in ['append', 'overwrite', 'error', 'ignore']\n\n        (df.write\n        .format(\"snowflake\") \n        .option(\"host\", self.host)\n        .option(\"user\", self.user)\n        .option(\"password\", self.password)\n        .option(\"sfWarehouse\", self.warehouse)\n        .option(\"database\", self.database)\n        .option(\"schema\", self.schema)\n        .option(\"dbtable\", self.table) \n        .mode(self.mode) \n        .save()\n        )\n"
    },
    {
        "block_type": "DatabricksWriter",
        "code_text": "from pyspark.sql import DataFrame\n\nfrom serra.writers import Writer\n\nclass DatabricksWriter(Writer):\n    \"\"\"\n    A writer to write data from a Spark DataFrame to a Databricks Delta table.\n\n    :param config: A dictionary containing the configuration for the writer.\n                   It should have the following keys:\n                   - 'database': The name of the Databricks database to write to.\n                   - 'table': The name of the table in the Databricks database.\n                   - 'format': The file format to use for the Delta table.\n                   - 'mode': The write mode to use, such as 'overwrite', 'append', etc.\n    \"\"\"\n\n    def __init__(self, database, table, format, mode):\n        self.database = database\n        self.table = table\n        self.format = format\n        self.mode = mode\n        \n    def write(self, df: DataFrame):\n        \"\"\"\n        Write data from a Spark DataFrame to a Databricks Delta table.\n\n        :param df: The Spark DataFrame to be written to the Delta table.\n        \"\"\"\n        # Currently forces overwrite if csv already exists\n        df.write.format(self.format).mode(self.mode).saveAsTable(f'{self.database}.{self.table}')\n        return None\n\n"
    },
    {
        "block_type": "S3Writer",
        "code_text": "from serra.writers import Writer\n\nclass S3Writer(Writer):\n    \"\"\"\n    A writer to write data from a Spark DataFrame to Amazon S3 as a CSV file.\n\n    :param config: A dictionary containing the configuration for the writer.\n                   It should have the following keys:\n                   - 'bucket_name': The name of the S3 bucket to write to.\n                   - 'file_path': The path to the CSV file in the S3 bucket.\n                   - 'file_type': The file type, which should be 'csv' for this writer.\n                   - 'input_block': The name of the input block used as a dependency for this writer.\n    \"\"\"\n    \n    def __init__(self, bucket_name, file_path, file_type, options, mode):\n        self.bucket_name = bucket_name\n        self.file_path = file_path\n        self.file_type = file_type\n        self.options = options\n        self.mode = mode\n    \n    def write(self, df):\n        \"\"\"\n        Write data from a Spark DataFrame to Amazon S3 as a CSV file.\n\n        :param df: The Spark DataFrame to be written.\n        \"\"\"\n        df_write = df.write\n\n        # To specify options like header: true\n        if self.options is not None:\n            for option_key, option_value in self.options.items():\n                df_write = df_write.option(option_key, option_value)\n        \n        s3_url = f\"s3a://{self.bucket_name}/{self.file_path}\"\n        df_write.mode(self.mode).format(self.file_type).save(s3_url)\n"
    },
    {
        "block_type": "AggregateTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\nclass AggregateTransformer(Transformer):\n    \"\"\"\n    A transformer to aggregate data based on specified columns and aggregation type.\n\n    :param group_by_columns: A list of column names to group the DataFrame by.\n    :param aggregation_type: The type of aggregation to be performed. Supported values: 'sum', 'avg', 'count'.\n    \"\"\"\n\n    def __init__(self, group_by_columns, aggregation_type):\n        self.group_by_columns = group_by_columns\n        self.aggregation_type = aggregation_type\n\n    def transform(self, df):\n        \"\"\"\n        Transform the DataFrame by aggregating data based on the specified configuration.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with the aggregated data.\n        \"\"\"\n        df = df.groupBy(*self.group_by_columns)\n\n        if self.aggregation_type == 'sum':\n            df = df.sum()\n        if self.type == 'avg':\n            df = df.mean()\n        if self.type == 'count':\n            df = df.count()\n\n        return df\n"
    },
    {
        "block_type": "MultipleCaseWhenTransformer",
        "code_text": "from pyspark.sql.functions import col, when\n\nfrom serra.transformers.transformer import Transformer\n\nclass MultipleCaseWhenTransformer(Transformer):\n    \"\"\"\n    A transformer that adds a new column to the DataFrame based on conditional rules (similar to SQL's CASE WHEN).\n\n    :param columns_and_conditions: A list of tuples representing the conditions and their corresponding results.\n                                   Each tuple should be in the format (condition_value, result_value).\n                                   The condition_value can be a specific value or a pattern for LIKE comparisons.\n                                   The result_value will be assigned to the output_col if the condition is met.\n    :param input_column: The name of the column containing the values to be evaluated.\n    :param output_col: The name of the new column to be added with the results of the conditions.\n    :param type: The type of comparison to be used. It can be either '==' for equality comparison or 'like' for pattern matching using the LIKE operator.\n    \"\"\"\n\n    def __init__(self, columns_and_conditions, input_column, output_col, type):\n        self.columns_and_conditions = columns_and_conditions\n        self.input_column = input_column\n        self.output_col = output_col\n        self.type = type\n\n    def transform(self, df):\n        \"\"\"\n        Add a new column with the results of the conditions to the DataFrame.\n\n        :param df: The input DataFrame.\n        :return: A new DataFrame with an additional column containing the results of the conditions.\n        \"\"\"\n        # Mappings to indicate which comparison function to use\n        type_dict = {\n            '<=': lambda col, val: col <= val,\n            '>=': lambda col, val: col >= val,\n            '==': lambda col, val: col == val,\n            'like': lambda col, val: col.like(val)\n        }\n\n        # Get the particular condition function based on the comparison type\n        comparison_func = type_dict.get(self.type)\n        output_cols = list(self.columns_and_conditions.keys())\n        conditions = list(self.columns_and_conditions.values())\n\n        if comparison_func is None:\n            raise ValueError(f\"Unsupported comparison type: {self.type}\")\n        \n        for i in range(len(output_cols)):\n            if len(conditions[i]) == 2:\n                case_expr = when(comparison_func(df[self.input_column[0]], conditions[i][0]), self.parse_result_value(conditions[i][1]))\n            else: \n                case_expr = when(comparison_func(df[self.input_column[0]], conditions[i][0]) | comparison_func(df[self.input_column[0]], conditions[i][2]), self.parse_result_value(conditions[i][1]))\n            case_expr = case_expr.otherwise(None)\n            df = df.withColumn(output_cols[i], case_expr)\n\n        return df\n    \n    def parse_result_value(self, result_value):\n        if 'col:' in result_value:\n            return col(result_value[4:])\n        else:\n            return(result_value)\n"
    },
    {
        "block_type": "MapTransformer",
        "code_text": "from pyspark.sql import functions as F\nimport json\nfrom serra.transformers.transformer import Transformer\nfrom serra.exceptions import SerraRunException\n\nclass MapTransformer(Transformer):\n    \"\"\"\n    A transformer to map values in a DataFrame column to new values based on a given mapping dictionary.\n\n    :param config: A dictionary containing the configuration for the transformer.\n                   It should have the following keys:\n                   - 'output_column': The name of the new column to be added after mapping.\n                   - 'mapping_dictionary': A dictionary containing the mapping of old values to new values.\n                                 If 'mapping_dictionary' is not provided, 'mapping_dict_path' should be specified.\n                   - 'mapping_dict_path': The path to a JSON file containing the mapping dictionary.\n                   - 'input_column': The name of the DataFrame column to be used as the key for mapping.\n    \"\"\"\n    def __init__(self, output_column, input_column, mapping_dictionary=None, mapping_dict_path=None):\n        self.output_column = output_column\n        self.input_column = input_column\n        self.mapping_dictionary = mapping_dictionary\n        self.mapping_dict_path = mapping_dict_path\n\n    def transform(self, df):\n        \"\"\"\n        Map values in the DataFrame column to new values based on the specified mapping.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with the new column containing the mapped values.\n        :raises: SerraRunException if any required config parameter is missing or if column specified\n                 as 'input_column' does not exist in the DataFrame.\n        \"\"\"\n        # if not self.output_column or not self.input_column:\n        #     raise SerraRunException(\"Both 'output_column' and 'input_column' must be provided in the config.\")\n        \n        if not self.mapping_dictionary and not self.mapping_dict_path:\n            raise SerraRunException(\"Either 'mapping_dictionary' or 'mapping_dict_path' must be provided in the config.\")\n\n        if self.input_column not in df.columns:\n            raise SerraRunException(f\"Column '{self.input_column}' specified as input_column does not exist in the DataFrame.\")\n\n        if self.mapping_dictionary is None:\n            with open(self.mapping_dict_path) as f:\n                self.mapping_dictionary = json.load(f)\n\n        try:\n            for key, value in self.mapping_dictionary.items():\n                df = df.withColumn(f'{self.output_column}_{key}', F.when(F.col(self.input_column) == key, value))\n\n            # Select the first non-null value from the generated columns\n            # create list, then unpack *\n            df = df.withColumn(self.output_column, F.coalesce(*[F.col(f'{self.output_column}_{key}') for key in self.mapping_dictionary]))\n            df = df.drop(*[f'{self.output_column}_{key}' for key in self.mapping_dictionary])\n        except Exception as e:\n            raise SerraRunException(f\"Error transforming DataFrame: {str(e)}\")\n\n        return df\n        \n        \n    \n\n        \n    "
    },
    {
        "block_type": "GetCountTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\nclass GetCountTransformer(Transformer):\n    \"\"\"\n    A transformer to get the count of occurrences of a column in the DataFrame.\n\n    :param group_by_columns: A list of column names to group the DataFrame by.\n    :param count_column: The name of the column for which the count is to be calculated.\n    \"\"\"\n\n    def __init__(self, group_by_columns, count_column):\n        self.group_by_columns = group_by_columns\n        self.count_column = count_column\n\n    def transform(self, df):\n        \"\"\"\n        Calculate the count of occurrences of the specified column in the DataFrame.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with the count of occurrences of the specified column\n                 for each group defined by the 'group_by' column(s).\n        \"\"\"\n\n        return df.groupBy(*self.group_by_columns).agg(F.count(self.count_column))\n    "
    },
    {
        "block_type": "GeoDistanceTransformer",
        "code_text": "from pyspark.sql import functions as F\nfrom pyspark.sql.types import FloatType\nfrom geopy.distance import geodesic\nfrom serra.transformers.transformer import Transformer\n\nclass GeoDistanceTransformer(Transformer):\n    \"\"\"\n    A transformer to calculate distances between pairs of geographical coordinates.\n\n    :param start_column: The name of the column containing user coordinates.\n    :param end_column: The name of the column containing facility coordinates.\n    :param distance_km_col: The name of the column to store the calculated distance in kilometers.\n    :param distance_mi_col: The name of the column to store the calculated distance in miles.\n    \"\"\"\n\n    def __init__(self, start_column, end_column, output_column):\n        self.start_column = start_column\n        self.end_column = end_column\n        self.output_column = output_column\n        \n    def transform(self, df):\n        \"\"\"\n        Calculate distances between pairs of coordinates and add them to the DataFrame.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with calculated distances added as new columns.\n        \"\"\"\n        # UDF to calculate distance in kilometers\n        def calculate_distance_km(user_coords, facility_coords):\n            user_lat, user_lon = map(float, user_coords.split(','))\n            facility_lat, facility_lon = map(float, facility_coords.split(','))\n            return geodesic((user_lat, user_lon), (facility_lat, facility_lon)).kilometers\n\n        calculate_distance_km_udf = F.udf(calculate_distance_km, FloatType())\n\n        # UDF to calculate distance in miles\n        def calculate_distance_mi(user_coords, facility_coords):\n            user_lat, user_lon = map(float, user_coords.split(','))\n            facility_lat, facility_lon = map(float, facility_coords.split(','))\n            return geodesic((user_lat, user_lon), (facility_lat, facility_lon)).miles\n\n        calculate_distance_mi_udf = F.udf(calculate_distance_mi, FloatType())\n\n        # Calculate distances and add them to the DataFrame\n        df_with_distances = df.withColumn(\n            self.output_column,\n            calculate_distance_mi_udf(df[self.start_column], df[self.end_column])\n        )\n\n        return df_with_distances"
    },
    {
        "block_type": "PivotTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\nfrom serra.exceptions import SerraRunException\n\nclass PivotTransformer(Transformer):\n    \"\"\"\n    A transformer to pivot a DataFrame based on specified row and column levels, and perform aggregation.\n\n    :param row_level_column: The column used for row levels during pivoting.\n    :param column_level_column: The column used for column levels during pivoting.\n    :param value_column: The column to be summarized (values) during pivoting.\n    :param aggregate_type: The type of aggregation to perform after pivoting.\n                           Should be one of 'avg' (average) or 'sum' (sum).\n    \"\"\"\n\n    def __init__(self, row_level_column, column_level_column, value_column, aggregate_type):\n        self.row_level_column = row_level_column\n        self.column_level_column = column_level_column\n        self.value_column = value_column\n        self.aggregate_type = aggregate_type\n\n    def transform(self, df):\n        \"\"\"\n        Pivot the DataFrame based on the specified row and column levels, and perform aggregation.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame resulting from the pivot operation with the specified aggregation.\n        :raises: SerraRunException if the specified aggregation type is invalid.\n        \"\"\"\n        \n        df = df.withColumn(self.value_column, F.col(self.value_column).cast(\"double\"))\n        df = df.groupBy(self.row_level_column).pivot(self.column_level_column)\n\n        # Perform aggregation\n        if self.aggregate_type == \"avg\":\n            df = df.avg(self.value_column)\n        elif self.aggregate_type == \"sum\":\n            df = df.sum(self.value_column)\n        else:\n            raise SerraRunException(\"Invalid Pivot Aggregation type\")\n\n        return df\n\n\n    "
    },
    {
        "block_type": "RenameColumnTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\nclass RenameColumnTransformer(Transformer):\n    \"\"\"\n    A transformer to rename a column in a DataFrame.\n\n    :param old_name: The name of the column to be renamed.\n    :param new_name: The new name to be assigned to the column.\n    \"\"\"\n\n    def __init__(self, old_name, new_name):\n        self.old_name = old_name\n        self.new_name = new_name\n\n    def transform(self, df):\n        \"\"\"\n        Rename a column in the DataFrame.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with the specified column renamed.\n        \"\"\"\n\n        df = df.withColumn(\n            self.new_name, F.col(self.old_name)\n        )\n        return df.drop(F.col(self.old_name))\n\n    "
    },
    {
        "block_type": "SqlTransformer",
        "code_text": "from pyspark.sql import DataFrame\nfrom pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\nclass SQLTransformer(Transformer):\n    \"\"\"\n    A transformer to perform a SQL operation on a DataFrame.\n\n    :param sql_expression: A SQL expression string representing the operation.\n    \"\"\"\n\n    def __init__(self, sql_expression):\n        self.sql_expression = sql_expression\n\n    def transform(self, df: DataFrame) -> DataFrame:\n        \"\"\"\n        Perform the SELECT operation on the DataFrame.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame containing only the selected columns.\n        :raises: SerraRunException if no columns are specified in the configuration\n                 or if none of the specified columns exist in the DataFrame.\n        \"\"\"\n        df = df.filter(F.expr(self.sql_expression))\n        return df\n\n\n\n"
    },
    {
        "block_type": "DateTruncTransformer",
        "code_text": "from pyspark.sql import functions as F\nfrom serra.transformers.transformer import Transformer\n\nclass DateTruncTransformer(Transformer):\n    \"\"\"\n    A transformer to truncate a timestamp column to a specified unit.\n\n    :param timestamp_column: The name of the timestamp column to be truncated.\n    :param trunc_unit: The unit for truncating the timestamp (e.g., 'day', 'month', 'year').\n    :param output_column: The name of the new column to create with the truncated timestamps.\n    \"\"\"\n\n    def __init__(self, timestamp_column, trunc_unit, output_column):\n        self.timestamp_column = timestamp_column\n        self.trunc_unit = trunc_unit\n        self.output_column = output_column\n\n    def transform(self, df):\n        \"\"\"\n        Truncate the specified timestamp column to the specified unit.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with the truncated timestamp column.\n        \"\"\"\n        dt = F.to_timestamp(self.timestamp_column, \"yyyy-MM-dd HH:mm:ss\")\n        \n        if self.trunc_unit == \"day\":\n            truncated_time = F.date_trunc(\"day\", dt)\n        elif self.trunc_unit == \"month\":\n            truncated_time = F.date_trunc(\"month\", dt)\n        elif self.trunc_unit == \"year\":\n            truncated_time = F.date_trunc(\"year\", dt)\n        else:\n            truncated_time = None\n        \n        return df.withColumn(self.output_column, truncated_time)\n"
    },
    {
        "block_type": "SelectTransformer",
        "code_text": "from pyspark.sql import DataFrame\nfrom pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\nfrom serra.exceptions import SerraRunException\n\nclass SelectTransformer(Transformer):\n    \"\"\"\n    A transformer to perform a SELECT operation on a DataFrame.\n\n    :param columns: A list of column names to select from the DataFrame.\n    :param distinct_column: Optional. The column for which distinct values should be retained.\n    :param filter_expression: Optional. A filter expression to apply to the DataFrame.\n    \"\"\"\n\n    def __init__(self, columns=None, distinct_column=None, filter_expression=None):\n        self.columns = columns or []\n        self.distinct_column = distinct_column\n        self.filter_expression = filter_expression\n\n    def transform(self, df: DataFrame) -> DataFrame:\n        \"\"\"\n        Perform the SELECT operation on the DataFrame.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame containing only the selected columns.\n        :raises: SerraRunException if no columns are specified in the configuration\n                 or if none of the specified columns exist in the DataFrame.\n        \"\"\"\n        if not self.columns:\n            columns = []\n\n        selected_columns = [F.col(col) for col in self.columns if col in df.columns]\n\n        if not selected_columns:\n            raise SerraRunException(\"None of the specified columns exist in the DataFrame.\")\n\n        df = df.select(*selected_columns)\n        \n        if self.distinct_column is not None:\n            df = df.dropDuplicates(self.distinct_column)\n\n        if self.filter_expression is not None:\n            df = df.filter(F.expr(self.filter_expression))\n        return df\n\n\n\n"
    },
    {
        "block_type": "CaseWhenTransformer",
        "code_text": "from pyspark.sql.functions import col,when \n\nfrom serra.transformers.transformer import Transformer\n\nclass CaseWhenTransformer(Transformer):\n    \"\"\"\n    Test transformer to add a column to dataframe\n    :param output_column: The name of the new output column to be added.\n    :param input_column: The name of the input column.\n    :param conditions: A list of conditions to evaluate. Each condition should be a tuple of (condition_column, operator, value).\n    :param comparison_type: The type of comparison to use (e.g., 'equals', 'greater_than').\n    :param is_column_condition: Boolean indicating whether the condition is based on a column value.\n    :param otherwise_value: The value to use when none of the conditions are met.\n    \"\"\"\n\n    def __init__(self, output_column, input_column, conditions, comparison_type, is_column_condition, otherwise_value):\n        self.output_column = output_column\n        self.input_column = input_column\n        self.conditions = conditions\n        self.comparison_type = comparison_type\n        self.is_column_condition = is_column_condition\n        self.otherwise_value = otherwise_value\n\n    def transform(self, df):\n        \"\"\"\n        Add column with col_value to dataframe\n        :return; Dataframe w/ new column containing col_value\n        \"\"\"\n        print(\"Conditions:\", self.conditions[0][0])\n        # Mappings to indicate which comparison function to use\n        type_dict = {\n            '<=': lambda col, val: col <= val,\n            '>=': lambda col, val: col >= val,\n            '==': lambda col, val: col == val,\n            'like': lambda col, val: col.like(val)\n        }\n\n        # Get the particular condition function based on the comparison type\n        comparison_func = type_dict.get(self.comparison_type)\n\n        if comparison_func is None:\n            raise ValueError(f\"Unsupported comparison type: {self.comparison_type}\")\n        print('###########', self.conditions[0][0])\n        # Create the 'when' expression based on the provided conditions and type\n        if self.is_column_condition is None:\n            case_expr = when(comparison_func(df[self.input_column], self.conditions[0][0]), self.parse_result_value(self.conditions[0][1]))\n            for cond_val, result_val in self.conditions[1:]:\n                case_expr = case_expr.when(comparison_func(df[self.input_column], cond_val), self.parse_result_value(result_val))\n\n            # Apply the 'otherwise' function to specify the default value (None for the last condition)\n            case_expr = case_expr.otherwise(None)\n\n            return df.withColumn(self.output_column, case_expr)\n        else:\n            case_expr = when(comparison_func(df[self.input_column], df[self.conditions[0][0]]), self.parse_result_value(self.conditions[0][1]))\n            for cond_val, result_val in self.conditions[1:]:\n                case_expr = case_expr.when(comparison_func(df[self.input_column], cond_val), self.parse_result_value(result_val))\n\n            # Apply the 'otherwise' function to specify the default value (None for the last condition)\n            case_expr = case_expr.otherwise(self.otherwise_value)\n\n            return df.withColumn(self.output_column, case_expr)\n    \n    def parse_result_value(self, result_value):\n        if isinstance(result_value,str) and 'col:' in result_value:\n            return col(result_value[4:])\n        else:\n            return(result_value)\n"
    },
    {
        "block_type": "ImputeTransformer",
        "code_text": "from pyspark.ml.feature import Imputer\nfrom pyspark.sql import functions as F\nfrom serra.transformers.transformer import Transformer\n\nclass ImputeTransformer(Transformer):\n    \"\"\"\n    A transformer to clean/impute missing values in specified columns.\n\n    :param columns_to_impute: A list of column names to impute missing values.\n    :param imputation_strategy: The imputation strategy. Supported values: 'mean', 'median', 'mode'.\n    \"\"\"\n\n    def __init__(self, columns_to_impute, imputation_strategy):\n        self.columns_to_impute = columns_to_impute\n        self.imputation_strategy = imputation_strategy\n\n    def transform(self, df):\n        \"\"\"\n        Add column with col_value to dataframe\n        :return; Dataframe w/ new column containing col_value\n        \"\"\"\n\n        imputer = Imputer(inputCols=self.columns_to_impute,\n                        outputCols=[\"{}_imputed\".format(c) for c in self.columns_to_impute]\n                        ).setStrategy(self.imputation_strategy)\n\n        df_imputed = imputer.fit(df).transform(df)\n        df_imputed = df_imputed.drop(*self.columns_to_impute)\n        \n        # Rename back to original columns\n        for c in self.columns_to_impute:\n            df_imputed = df_imputed.withColumnRenamed(\"{}_imputed\".format(c), c)\n\n        return df_imputed\n"
    },
    {
        "block_type": "AddColumnTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\nfrom serra.exceptions import SerraRunException\n\nclass AddColumnTransformer(Transformer):\n    \"\"\"\n    A transformer to add a new column to the DataFrame with a specified value.\n\n    :param new_column_name: The name of the new column to be added.\n    :param value: The value to be set for the new column.\n    :param new_column_type: The data type of the new column. Must be a valid PySpark data type string.\n    \"\"\"\n\n    def __init__(self, new_column_name, value, new_column_type):\n        self.new_column_name = new_column_name\n        self.value = value\n        self.new_column_type = new_column_type\n        \n    def transform(self, df):\n        \"\"\"\n        Add a new column to the DataFrame with the specified name, value, and data type.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with the new column added.\n        :raises: SerraRunException if the column with the specified name already exists in the DataFrame.\n        \"\"\"\n        if self.new_column_name in df.columns:\n            raise SerraRunException(f\"Column '{self.new_column_name}' already exists in the DataFrame. Choose a different name.\")\n        \n        return df.withColumn(\n            self.new_column_name, F.lit(self.value).cast(self.new_column_type)  \n        )\n    "
    },
    {
        "block_type": "JoinTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.exceptions import SerraRunException\nfrom serra.transformers.transformer import Transformer\n\nclass JoinTransformer(Transformer):\n    \"\"\"\n    A transformer to join two DataFrames together based on a specified join condition.\n\n    :param config: A dictionary containing the configuration for the transformer.\n                   It should have the following keys:\n                   - 'join_type': The type of join to perform. Currently only 'inner' join is supported.\n                   - 'join_on': A dictionary where the keys are the table names (or DataFrame aliases)\n                                and the values are the column names to join on for each table.\n                   Example: {'table1': 'column1', 'table2': 'column2'}\n    \"\"\"\n\n\n    def __init__(self, join_type, join_on):\n        self.join_type = join_type\n        self.join_on = join_on\n    \n    @property\n    def dependencies(self):\n        return self.input_block\n\n    def transform(self, df1, df2):\n        \"\"\"\n        Join two DataFrames together based on the specified join condition.\n\n        :param df1: The first DataFrame to be joined.\n        :param df2: The second DataFrame to be joined.\n        :return: A new DataFrame resulting from the join operation.\n        :raises: SerraRunException if the join condition columns do not match between the two DataFrames.\n        \"\"\"\n        # assert self.join_type in \"inner\"\n\n        join_keys = []\n\n        \n        for table in self.join_on:\n            join_keys.append(self.join_on.get(table))\n\n        assert len(join_keys) == 2\n\n        matching_col = join_keys\n\n        df1 = df1.join(df2, df1[matching_col[0]] == df2[matching_col[1]], self.join_type).drop(df2[matching_col[1]])\n        if df1.isEmpty():\n            raise SerraRunException(f\"\"\"Joiner - Join Key Error: {matching_col[0]}, {matching_col[1]} columns do not match.\"\"\")\n        return df1\n    \n    "
    },
    {
        "block_type": "WindowTransformer",
        "code_text": "from pyspark.sql import DataFrame\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window\nfrom serra.transformers.transformer import Transformer\n\nclass WindowTransformer(Transformer):\n    \"\"\"\n    A transformer to set a window for all other steps after it.\n\n    :param partition_by: A list of column names for partitioning the window.\n    :param order_by: A list of column names for ordering within the window.\n    :param window_name: The name of the window column to be added.\n    \"\"\"\n\n    def __init__(self, partition_by=None, order_by=None, window_name=None):\n        self.partition_by = partition_by or []\n        self.order_by = order_by or []\n        self.window_name = window_name\n\n    def transform(self, df: DataFrame) -> DataFrame:\n        window_spec = Window().partitionBy(*self.partition_by).orderBy(*self.partition_by)\n        df_with_window = df.withColumn(f\"window_{self.partition_by[0]}\", F.row_number().over(window_spec))\n        return df_with_window\n"
    },
    {
        "block_type": "CrossJoinTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.exceptions import SerraRunException\nfrom serra.transformers.transformer import Transformer\n\nclass CrossJoinTransformer(Transformer):\n    \"\"\"\n    A transformer to join two DataFrames together\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @property\n    def dependencies(self):\n        return self.input_block\n\n    def transform(self, df1, df2):\n        \"\"\"\n        Join two DataFrames together based on the specified join condition.\n\n        :param df1: The first DataFrame to be joined.\n        :param df2: The second DataFrame to be joined.\n        :return: A new DataFrame resulting from the join operation.\n        :raises: SerraRunException if the join condition columns do not match between the two DataFrames.\n        \"\"\"\n        \n        return df1.alias('a').crossJoin(df2.alias('b'))\n    \n    "
    },
    {
        "block_type": "FilterTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\nclass FilterTransformer(Transformer):\n    \"\"\"\n    A transformer to filter the DataFrame based on a specified condition.\n\n    :param filter_values: A list of values to filter the DataFrame by.\n    :param filter_column: The name of the column to apply the filter on.\n    :param is_expression: Whether the filter is an expression (boolean condition).\n    \"\"\"\n\n    def __init__(self, filter_values, filter_column, is_expression=False):\n        self.filter_values = filter_values\n        self.filter_column = filter_column\n        self.is_expression = is_expression\n    \n    def transform(self, df):\n        \"\"\"\n        Filter the DataFrame based on the specified condition.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with rows filtered based on the specified condition.\n        \"\"\"\n        if self.is_expression == None:\n            self.is_expression = False\n        if self.is_expression:\n            return df.filter(F.expr(self.filter_values))\n\n        return df.filter(df[self.filter_column].isin(self.filter_values))\n"
    },
    {
        "block_type": "MultiJoinTransformer",
        "code_text": "from pyspark.sql.functions import col\n\nfrom serra.exceptions import SerraRunException\nfrom serra.transformers.transformer import Transformer\n\nclass MultiJoinTransformer(Transformer):\n    \"\"\"\n    A transformer to join multiple DataFrames together based on specified join conditions.\n\n    :param join_type: The type of join to perform. Currently only 'inner' join is supported.\n    :param join_on: A dictionary where the keys are the table names (or DataFrame aliases)\n                    and the values are the column names to join on for each table.\n                    Example: {'table1': 'column1', 'table2': 'column2'}\n    \"\"\"\n\n    def __init__(self, join_type, join_on):\n        self.join_type = join_type\n        self.join_on = join_on\n\n    def transform(self, *dfs):\n        \"\"\"\n        Join multiple DataFrames together based on the specified join conditions.\n\n        :param dfs: A variable number of DataFrames to be joined.\n        :return: A new DataFrame resulting from the join operations.\n        :raises: SerraRunException if the join condition columns do not match between the DataFrames.\n        \"\"\"  \n\n        join_keys = []\n        for table in self.join_on:\n            join_keys.append(self.join_on.get(table))\n            \n\n        matching_col = join_keys\n\n        joined_df = dfs[0]\n        for i in range(1,len(dfs)):\n            if matching_col[i-1] != matching_col[i]:\n                joined_df = joined_df.join(dfs[i], joined_df[matching_col[i-1]] == dfs[i][matching_col[i]], self.join_type[i-1])\n            else:\n                joined_df = joined_df.join(dfs[i], matching_col[i], self.join_type[i-1])\n\n            non_null_counts = [joined_df.where(col(c).isNotNull()).count() for c in dfs[i].columns]\n            if joined_df.isEmpty() or all(count == 0 for count in non_null_counts):\n                raise SerraRunException(f\"\"\"Joiner - Join Key Error: {matching_col[0]}, {matching_col[1]} columns do not match.\"\"\")\n\n        return joined_df\n"
    },
    {
        "block_type": "CastColumnsTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\nclass CastColumnsTransformer(Transformer):\n    \"\"\"\n    A transformer to cast columns in a DataFrame to specified data types.\n\n    :param columns_to_cast: A dictionary where the keys are the target column names\n                            and the values are lists containing the source column name\n                            and the target data type to which the source column will be cast.\n                            Example: {'target_column': ['source_column', 'target_data_type']}\n    \"\"\"\n\n    def __init__(self, columns_to_cast):\n        self.columns_to_cast = columns_to_cast\n\n    def transform(self, df):\n        \"\"\"\n        Cast columns in the DataFrame to specified data types.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with the specified columns cast to the target data types.\n        \"\"\"\n        for target, [source, target_data_type] in self.columns_to_cast.items():\n            df = df.withColumn(target, F.col(source).cast(target_data_type))\n\n        return df"
    },
    {
        "block_type": "CoalesceTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\nclass CoalesceTransformer(Transformer):\n    \"\"\"\n    A transformer to create a new column by coalescing multiple columns.\n\n    :param input_columns: A list of column names to coalesce.\n    :param output_column: The name of the new column to create with the coalesced values.\n    \"\"\"\n\n    def __init__(self, input_columns, output_column):\n        self.input_columns = input_columns\n        self.output_column = output_column\n\n    def transform(self, df):\n        \"\"\"\n        Create a new column by coalescing multiple columns.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with the coalesced column.\n        \"\"\"\n\n        return df.withColumn(self.output_column, F.coalesce(*self.input_columns))\n"
    },
    {
        "block_type": "CustomTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\n## helpers to execute custom transform code\nimport re\nimport importlib\n\ndef get_user_transform(code_string, desired_function_name):\n    # Extract and perform imports\n    imports = re.findall(r\"from ([\\w\\.]+) import ([\\w ,]+)\", code_string)\n    for imp in imports:\n        module_name, components = imp\n        module = importlib.import_module(module_name)\n        for component in components.split(\",\"):\n            component = component.strip()\n            if \" as \" in component:\n                actual_name, alias = component.split(\" as \")\n                globals()[alias] = getattr(module, actual_name)\n            else:\n                globals()[component] = getattr(module, component)\n\n    # Execute the provided code string\n    local_namespace = {}\n    exec(code_string, globals(), local_namespace)\n    \n    # Rename the transform function\n    globals()[desired_function_name] = local_namespace[\"transform\"]\n\nclass CustomTransformer(Transformer):\n\n    def __init__(self, code_block):\n        self.code_block = code_block\n\n    def transform(self, df):\n        custom_transform_func_name = \"custom_transform_\" + str(id(self))\n        get_user_transform(self.code_block, custom_transform_func_name)\n        output_df = globals()[custom_transform_func_name](df)\n        return output_df"
    },
    {
        "block_type": "OrderByTransformer",
        "code_text": "from serra.transformers.transformer import Transformer\n\nclass OrderByTransformer(Transformer):\n    \"\"\"\n    A transformer to sort the DataFrame based on specified columns in ascending or descending order.\n\n    :param columns: A list of column names to sort the DataFrame by.\n    :param ascending: Optional. If True (default), sort in ascending order. If False, sort in descending order.\n    \"\"\"\n\n    def __init__(self, columns, ascending=True):\n        self.columns = columns\n        self.ascending = ascending\n\n    def transform(self, df):\n        \"\"\"\n        Transform the DataFrame by sorting it based on specified columns.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with the rows sorted based on the specified columns.\n        \"\"\"\n\n        return df.orderBy(*self.columns, ascending = self.ascending)\n"
    },
    {
        "block_type": "DropColumnsTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\nclass DropColumnsTransformer(Transformer):\n    \"\"\"\n    A transformer to drop specified columns from the DataFrame.\n\n    :param config: A dictionary containing the configuration for the transformer.\n                   It should have the following key:\n                   - 'columns_to_drop': A list of column names to be dropped from the DataFrame.\n    \"\"\"\n\n    def __init__(self, columns_to_drop):\n        self.columns_to_drop = columns_to_drop\n\n    def transform(self, df):\n        \"\"\"\n        Drop specified columns from the DataFrame.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with the specified columns dropped.\n        \"\"\"\n        return df.select([c for c in df.columns if c not in self.columns_to_drop])\n    "
    },
    {
        "block_type": "JoinWithConditionTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\nclass JoinWithConditionTransformer(Transformer):\n    \"\"\"\n    A transformer to join two DataFrames together based on a specified join condition.\n\n    :param join_type: The type of join to perform. Currently only 'inner' join is supported.\n    :param condition: The condition for the join.\n    :param join_on: A dictionary where the keys are the table names (or DataFrame aliases)\n                    and the values are the column names to join on for each table.\n                    Example: {'table1': 'column1', 'table2': 'column2'}\n    \"\"\"\n\n    def __init__(self, join_type, condition, join_on):\n        self.join_type = join_type\n        self.condition = condition\n        self.join_on = join_on\n\n    def transform(self, df1, df2):\n        \"\"\"\n        Join two DataFrames together based on the specified join condition.\n\n        :param df1: The first DataFrame to be joined.\n        :param df2: The second DataFrame to be joined.\n        :return: A new DataFrame resulting from the join operation.\n        :raises: SerraRunException if the join condition columns do not match between the two DataFrames.\n        \"\"\"\n        \n        df1 = df1.alias('a').join(df2.alias('b'), F.expr(self.condition), self.join_type)\n\n        return df1\n    \n    "
    },
    {
        "block_type": "GetDistinctTransformer",
        "code_text": "from pyspark.sql import functions as F\n\nfrom serra.transformers.transformer import Transformer\n\nclass GetDistinctTransformer(Transformer):\n    \"\"\"\n    A transformer to drop duplicate rows from the DataFrame based on specified column(s).\n\n    :param columns_to_check: A list of column names to identify rows for duplicates removal.\n    \"\"\"\n\n    def __init__(self, columns_to_check):\n        self.columns_to_check = columns_to_check\n\n    def transform(self, df):\n        \"\"\"\n        Drop duplicate rows from the DataFrame based on specified columns.\n\n        :param df: The input DataFrame to be transformed.\n        :return: A new DataFrame with duplicate rows removed based on the specified columns.\n        \"\"\"\n\n        return df.dropDuplicates(self.columns_to_check)\n"
    },
    {
        "block_type": "GetMaxOrMinTransformer",
        "code_text": "from serra.transformers.transformer import Transformer\nfrom pyspark.sql.functions import lit,max,min\nfrom pyspark.sql import Window\n\nclass GetMaxOrMinTransformer(Transformer):\n    \"\"\"\n    Test transformer to add a column to the dataframe with the maximum or minimum value from another column.\n\n    :param columns_and_operations: A dictionary specifying the column to be transformed and the operation to be performed.\n                                   The dictionary format should be {'input_column': 'aggregation_type'}.\n                                   The output_column will contain the result of the aggregation operation.\n    :param new_column_names: A dictionary specifying the names of the new columns to create for each aggregation operation.\n                             The dictionary format should be {'aggregation_type': 'new_column_name'}.\n    :param group_by_columns: A list of column names to group the DataFrame by.\n    \"\"\"\n\n    def __init__(self, columns_and_operations, new_column_names, group_by_columns):\n        self.columns_and_operations = columns_and_operations\n        self.new_column_names = new_column_names\n        self.group_by_columns = group_by_columns\n\n    def transform(self, df):\n        \"\"\"\n        Add a column with the maximum or minimum value to the DataFrame.\n\n        :param df: The input DataFrame.\n        :return: A new DataFrame with an additional column containing the maximum or minimum value.\n        \"\"\"\n        window_spec = Window.partitionBy(self.group_by_columns)\n\n        i = 0\n        for input_col_name, agg_type in self.columns_and_operations.items():\n            if agg_type == 'max':\n                result_col = max(input_col_name).over(window_spec)\n            elif agg_type == 'min':\n                result_col = min(input_col_name).over(window_spec)\n            \n            df = df.withColumn(self.new_column_names[i], result_col)\n            i+=1\n        \n        return df\n\n"
    }
]